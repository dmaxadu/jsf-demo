<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">How to configure WildFly with YAML files</title><link rel="alternate" href="https://www.mastertheboss.com/jbossas/jboss-configuration/how-to-configure-wildfly-with-yaml-files/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/jbossas/jboss-configuration/how-to-configure-wildfly-with-yaml-files/</id><updated>2023-04-11T12:40:48Z</updated><content type="html">WildFly 28 includes support for YAML configuration which is offers a more flexible approach in some use cases. In this tutorial we will discuss which are the best scenarios where YAML configuration is a perfect fit and how to configure WildFly to use YAML files Why YAML Files? One of the main advantages of YAML ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Deploy React applications to OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/11/deploy-react-apps-openshift" /><author><name>Yashwanth Maheshwaram</name></author><id>4a539055-118b-4d0e-b83e-334df1be8d12</id><updated>2023-04-11T07:00:00Z</updated><published>2023-04-11T07:00:00Z</published><summary type="html">&lt;p&gt;React is an open-source &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; library for building user interfaces. It allows developers to create reusable UI components and efficiently update the view in response to changes in data. &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; enables developers to build, deploy, run, and manage a wide variety of applications, including frontend and the ones made with React. React applications use the &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; runtime to run the application.&lt;/p&gt; &lt;p&gt;This article will help you get started with ReactJS apps on OpenShift. You will learn how to:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Deploy a basic React application from the ground up in the easiest way possible without having to deal with &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; and a lot of other complications that come with it.&lt;/li&gt; &lt;li aria-level="1"&gt;Manage product and development environments for a React application.&lt;/li&gt; &lt;li aria-level="1"&gt;Add &lt;a href="developers.redhat.com/topics/ci-cd"&gt;continuous deployment (CD)&lt;/a&gt; to your React application to automatically deploy updates to your repository.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;Node.js is required for your machine to be able to build and run React applications on your local machine. Install &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt;  via one of the following options: &lt;/p&gt; &lt;ul&gt;&lt;li aria-level="2"&gt;If you are using &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL): &lt;a href="https://access.redhat.com/products/nodejs"&gt;https://access.redhat.com/products/nodejs&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;If you're using any other operating system: &lt;a href="https://nodejs.org/en/"&gt;https://nodejs.org/en/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Create a sample React application&lt;/h2&gt; &lt;p&gt;Create React App is a comfortable environment for learning React, and it is the best way to start building a new single-page application in React. It sets up your development environment so that you can use the latest JavaScript features, provides a nice developer experience, and optimizes your app for production.&lt;/p&gt; &lt;p&gt;You’ll need to have Node 14.0.0+ and npm 5.6+ on your machine. If you want to use a readily available example application, skip this step and move to the Deploy section.&lt;/p&gt; &lt;p&gt;To create a project, run:&lt;/p&gt; &lt;pre class="hljs"&gt; npx create-react-&lt;span class="hljs-keyword"&gt;app my-&lt;span class="hljs-keyword"&gt;app &lt;span class="hljs-keyword"&gt;cd my-&lt;span class="hljs-keyword"&gt;app npm start&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt; &lt;h2&gt;Publish your code to Git&lt;/h2&gt; &lt;p&gt;Create a repository on GitHub or any other Git platform. If you want to use a &lt;a href="https://github.com/yashwanthm/react-openshift-example"&gt;readily available example application&lt;/a&gt;, copy the URL and move on to the next step: &lt;code&gt;https://github.com/yashwanthm/react-openshift-example&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The repository has &lt;a href="https://create-react-app.dev/docs/deployment#customizing-environment-variables-for-arbitrary-build-environments"&gt;a minor recommended change to the usual create-react-app approach&lt;/a&gt; to be able to run a production application.&lt;/p&gt; &lt;p&gt;You will need to run &lt;code&gt;npm run start.development.local&lt;/code&gt; instead of &lt;code&gt;npm start&lt;/code&gt; on your local machine. The rest remains the same.&lt;/p&gt; &lt;h2&gt;Deploy your React application to the OpenShift sandbox&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; is a free-to-use OpenShift instance for you to experiment with OpenShift for your use cases. It's an excellent way to try running React applications on OpenShift. &lt;/p&gt; &lt;p&gt;Follow these steps to start your sandbox instance and deploy your app:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Create a Sandbox account using &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;https://developers.redhat.com/developer-sandbox&lt;/a&gt; &lt;/li&gt; &lt;li aria-level="1"&gt;Once you have the account, click on &lt;strong&gt;Start using your sandbox.&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Give it a few seconds and your sandbox instance will load up.&lt;/li&gt; &lt;li aria-level="1"&gt;On the left side menu, click on &lt;strong&gt;+Add&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Select &lt;strong&gt;Import from Git&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Specify your Git repo URL: &lt;code&gt;https://github.com/yashwanthm/react-openshift-example &lt;/code&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;You will now be moved to the Topology view, and the app will start to deploy. Give it about a minute to finish deployment. While it’s deploying, you will be able to view the logs.&lt;/li&gt; &lt;li aria-level="1"&gt;Once it’s done deploying, you can click on the OpenURL button to see the UI of your React application running on OpenShift.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Congratulations! You now have a vanilla React application that runs on OpenShift without having to work with complex configurations that are needed. Read on for details on how to make a change and see the change go live for your application.&lt;/p&gt; &lt;h2&gt;Fork and deploy your own repo&lt;/h2&gt; &lt;p&gt;Now that you tried using the example repository, follow these steps to deploy your own repo:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Fork &lt;code&gt;https://github.com/yashwanthm/react-openshift-example&lt;/code&gt; or create your own repo.&lt;/li&gt; &lt;li aria-level="1"&gt;Copy the URL.&lt;/li&gt; &lt;li aria-level="1"&gt;Log in to the Developer Sandbox.&lt;/li&gt; &lt;li aria-level="1"&gt;Import from Git and use your repository to create an application.&lt;/li&gt; &lt;li aria-level="1"&gt;Access your application.&lt;/li&gt; &lt;li aria-level="1"&gt;Add CD.&lt;/li&gt; &lt;li aria-level="1"&gt;Make changes to the source &lt;code&gt;App.js&lt;/code&gt;, commit, and push.&lt;/li&gt; &lt;li aria-level="1"&gt;See your changes get deployed automatically.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Add continuous deployment&lt;/h2&gt; &lt;p&gt;Continuous deployment is a strategy in software development where code changes to an application are released automatically into the production environment. It speeds up time to market by eliminating the lag between coding and customer value. OpenShift enables developers to configure this using a few simple UI-based steps.&lt;/p&gt; &lt;p&gt;Now, let’s begin adding CD to the application we just created:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Click on Actions.&lt;/li&gt; &lt;li aria-level="1"&gt;Select &lt;strong&gt;Edit &lt;application name&gt;&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Check the &lt;strong&gt;Add pipeline&lt;/strong&gt; checkbox. You can see the pipeline visualization if you’d like to understand the steps.&lt;/li&gt; &lt;li aria-level="1"&gt;Click &lt;strong&gt;Save&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; OpenShift Sandbox will suspend your application when it is idled but will bring back the pod up when there’s a hit to the URL, it takes a few seconds for the application to load. However, this will not be the case on your production OpenShift instance. &lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;OpenShift provides a simplified developer experience for running the applications on the cloud. Explore other popular activities for the sandbox:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/developer-sandbox/activities/how-to-deploy-java-application-in-kubernetes"&gt;Deploy a Java application on Kubernetes in minutes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/developer-sandbox/activities/migrate-and-deploy-cloud-foundry"&gt;Migrate and deploy Cloud Foundry applications to Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/node/283511"&gt;Run the Canary Deployment pattern on Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/11/deploy-react-apps-openshift" title="Deploy React applications to OpenShift"&gt;Deploy React applications to OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Yashwanth Maheshwaram</dc:creator><dc:date>2023-04-11T07:00:00Z</dc:date></entry><entry><title>How to deploy single sign-on as code using GitOps</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/10/how-deploy-single-sign-code-using-gitops" /><author><name>Pablo Castelo, Ignacio Lago</name></author><id>163fbb25-7860-4209-9503-300b78ec94e6</id><updated>2023-04-10T07:00:00Z</updated><published>2023-04-10T07:00:00Z</published><summary type="html">&lt;p&gt;In this series of articles, we will demonstrate how to use Git, Argo CD, and Red Hat OpenShift GitOps to build a continuous delivery cycle that automatically synchronizes and deploys different solutions. In this article, we will discuss the &lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;single sign-on for Red Hat solutions&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In the world of &lt;a href="https://kubernetes.io/"&gt;Kubernetes&lt;/a&gt;, where simplicity is tied to complexity, sometimes you can deploy your application with flexible horizontal auto-scaling, out-of-the-box load balancing, distributed management of components, and centralized control of multiple applications. However, with great power comes great responsibilities and complexity.&lt;/p&gt; &lt;p&gt;To help us solve this complexity and take accountability for our newfound power, strategies have been developed for Kubernetes. In this article, we will take a closer look at &lt;a href="http://developers.redhat.com/topics/ci-cd"&gt;continuous integration and continuous deployment (CI/CD)&lt;/a&gt;. These systems usually work with a high level of abstraction to help us solve four common issues: version control, change logging, consistency of deployments, and rollback functionality. One of the most popular approaches to this abstraction layer is called &lt;a href="https://www.redhat.com/en/topics/devops/what-is-gitops"&gt;GitOps&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;GitOps, originally proposed in a Weaveworks &lt;a href="https://www.weave.works/blog/gitops-operations-by-pull-request"&gt;blog post&lt;/a&gt; in 2017, is built around &lt;a href="https://git-scm.com/"&gt;Git&lt;/a&gt; implementation. It is a “single source of truth” in CI/CD processes.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/container-platform"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; is the leading &lt;a href="https://kubernetes.io/"&gt;Kubernetes&lt;/a&gt; platform in GitOps deployments. It comes out of the box with access to &lt;a href="https://operatorframework.io/"&gt;operators&lt;/a&gt; (curated and supported by &lt;a href="https://www.redhat.com"&gt;Red Hat&lt;/a&gt;) that are part of the &lt;a href="https://catalog.redhat.com/software/operators/detail/5fb288c70a12d20cbecc6056"&gt;OpenShift GitOps Operator&lt;/a&gt;. &lt;a href="https://argoproj.github.io/argo-cd"&gt;Argo CD&lt;/a&gt;, a declarative continuous delivery tool, is also part of this.&lt;/p&gt; &lt;p&gt;By having comprehensive management of the deployment and lifecycle of things, it provides solutions for version control, configurations, and application definitions in Kubernetes environments, organizing complex data with an easy-to-understand user interface.&lt;/p&gt; &lt;p&gt;Argo CD has support for common ways of deploying Kubernetes manifests such as &lt;a href="https://kustomize.io/"&gt;Kustomize&lt;/a&gt; applications, &lt;a href="https://helm.sh/"&gt;Helm&lt;/a&gt; charts, and regular YAML/JSON files.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To start this demonstration, you will need the following:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Red Hat OpenShift cluster&lt;/li&gt; &lt;li aria-level="1"&gt;Admin user&lt;/li&gt; &lt;li aria-level="1"&gt;The tools listed in this table:&lt;/li&gt; &lt;/ul&gt;&lt;div&gt; &lt;table cellspacing="0" width="625"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;&lt;strong&gt;Tools Required&lt;/strong&gt;&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;macOS&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Linux/Fedora&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;Git&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;&lt;a href="https://git-scm.com/download/mac"&gt;Download&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;&lt;a href="https://git-scm.com/download/linux"&gt;Download&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;OpenShift client 4.11&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;&lt;a href="https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.11.11/openshift-client-linux-4.11.11.tar.gz"&gt;Download&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;&lt;a href="https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.11.11/openshift-client-mac-4.11.11.tar.gz"&gt;Download&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;Argo CD CLI&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;&lt;a href="https://argo-cd.readthedocs.io/en/stable/cli_installation/#download-with-curl"&gt;Download&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;&lt;a href="https://argo-cd.readthedocs.io/en/stable/cli_installation/#mac"&gt;Download&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; These tools are available through the &lt;a href="https://docs.openshift.com/container-platform/4.10/web_console/odc-about-web-terminal.html"&gt;Web Terminal Operator&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Install OpenShift GitOps operator and Argo CD instance&lt;/h2&gt; &lt;p&gt;For the scope of this article, we will use the following &lt;a href="https://github.com/ignaciolago/keycloak-gitops"&gt;Git repository&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone https://github.com/ignaciolago/keycloak-gitops cd keycloak-gitops&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To install the OpenShift GitOps operator and Argo CD instance, take a look a the files first as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cat 00_argocd_namespace.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We will use this file to create a namespace for our installation:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: v1 kind: Namespace #kubernetes api element kind for namespace metadata: #argocd wave in which the element is synced name: openshift-gitops #default name for the installation in Openshift spec: {}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Enter the following command for the subscription to install the GitOps operator:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cat 01_argocd_subscription.yaml&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: operators.coreos.com/v1alpha1 kind: Subscription # subscription for the Operator, would add it to the operator # this operator installs an argocd instance for default metadata: name: openshift-gitops-operator namespace: openshift-operators spec: channel: latest installPlanApproval: Automatic name: openshift-gitops-operator source: redhat-operators sourceNamespace: openshift-marketplace # startingCSV: openshift-gitops-operator.v1.7.2&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The role binding for the Argo CD instance:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cat 02_argocd_rbac.yaml&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 # role base access control binding for argocd permissions metadata: name: argocd-rbac-ca subjects: - kind: ServiceAccount # tied to the argocd service account name: openshift-gitops-argocd-application-controller # since we are using applications we use the argocd application controller namespace: openshift-gitops roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin # in this example we are using cluster-admin but we can give it lesser permissions if needed&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We are using Kustomize to package because it helps with debugging and allows us to reutilize code for multiple environments. We will use this in subsequent articles. We have a &lt;code&gt;kustomization.yaml&lt;/code&gt; file containing the references for all the other files as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cat customization.yaml&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - 00_argocd_namespace.yaml - 01_argocd_subscription.yaml - 02_argocd_rbac.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now that we understand the files, we will proceed with installing the OpenShift GitOps operator and Argo CD instance. Use the Kustomize flag, &lt;code&gt;-k&lt;/code&gt; or &lt;code&gt;–kustomize&lt;/code&gt; as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -k bootstrap/argocd&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As we can see in the following output, the previous command will create three resources for us, the namespace, the role-binding, and the subscription and installation of the operator.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;namespace/openshift-gitops created clusterrolebinding.rbac.authorization.k8s.io/argocd-rbac-ca created subscription.operators.coreos.com/openshift-gitops-operator created&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now we are going to verify that the OpenShift GitOps operator and Argo CD instance components are installed by using the following command to verify the status of the deployments:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ watch oc get pods -n openshift-gitops&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;NAME READY STATUS RESTARTS AGE cluster-54f5bcdc85-5vs57 1/1 Running 0 4m5s kam-7d7bfc8675-xmcdg 1/1 Running 0 4m4s openshift-gitops-application-controller-0 1/1 Running 0 4m2s openshift-gitops-applicationset-controller-5cf7bb9dbc-8qtzl 1/1 Running 0 4m2s openshift-gitops-dex-server-6575c69849-knt27 1/1 Running 0 4m2s openshift-gitops-redis-bb656787d-wglb7 1/1 Running 0 4m2s openshift-gitops-repo-server-54c7998dbf-tglrc 1/1 Running 0 4m2s openshift-gitops-server-786849cbb8-l9npk 1/1 Running 0 4m2s&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Of course, we can also check how the installation is going in the GUI by accessing the OpenShift web console and the Operator Hub (Figure 1):&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image5_2.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image5_2.png?itok=YOExWTLB" width="600" height="363" alt="Installed Operators -&gt; GitOps Operator" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: Check the GitOps operator installation.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now that the operator is installed, we recover the path of the Argo CD GUI by using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get route openshift-gitops-server -n openshift-gitops --template='https://{{.spec.host}}' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Copy the following URL and paste it into the browser to access the Argo CD login page.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;https://openshift-gitops-server-openshift-gitops.apps.cluster-lvn9g.lvn9g.sandbox1571.opentlc.com​&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Click the &lt;strong&gt;log in via OpenShift&lt;/strong&gt; button.&lt;/p&gt; &lt;p&gt;Then enter your OpenShift credentials.&lt;/p&gt; &lt;p&gt;Proceed to authorize access to Argo CD to the user.&lt;/p&gt; &lt;p&gt;Now that we have added the permissions to our user, we are able to see the Argo CD interface. Now it's time to learn how to deploy applications using it.&lt;/p&gt; &lt;h2&gt;Deploying single sign-on for a dev environment&lt;/h2&gt; &lt;p&gt;For this part, we are going to learn how to automatically install all the components needed to run &lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;single sign-on&lt;/a&gt; (SSO) using GitOps and a Git repository as the source of truth.&lt;/p&gt; &lt;p&gt;We are going to install a namespace-contained installation of SSO using the operator deploying a managed SSO and Postgres installation; for that, let us take a look at the files for this we do:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd../../resources/01_rhsso-dev&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We have the namespace yaml like before:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cat 00_namespace_rhsso-dev.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Again we create the namespace, but this time for SSO:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: v1 kind: Namespace metadata: annotations: argocd.argoproj.io/sync-wave: "0" # now that we deploying using ArgoCD we can specify the order of deploy name: rhsso-dev spec: {}&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cat 01_rhsso-operator_resourcegroups.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Operator group for SSO:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: annotations: argocd.argoproj.io/sync-wave: "1" # Sync Wave 1 since we need to have the namespace first name: rhsso-dev namespace: rhsso-dev spec: targetNamespaces: - rhsso-dev&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cat 02_rhsso-operator.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now we will create a subscription for the installation of the operator.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: annotations: argocd.argoproj.io/sync-wave: "2" # sync wave 2 since we need the Namespace and OperatorGroup and so on... name: rhsso-operator namespace: rhsso-dev spec: channel: stable installPlanApproval: Automatic name: rhsso-operator source: redhat-operators sourceNamespace: openshift-marketplace startingCSV: rhsso-operator.7.6.1-opr-005&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cat 03_deploy_rhsso-dev.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This file contains the config of the instance that we are going to create inside the operator to install the product.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: keycloak.org/v1alpha1 kind: Keycloak metadata: annotations: argocd.argoproj.io/sync-wave: "3" name: rhsso-dev labels: app: rhsso-dev namespace: rhsso-dev spec: multiAvailablityZones: enabled: true # we add this flag for HA deploy, we need more than 1 pod for this externalAccess: enabled: true keycloakDeploymentSpec: imagePullPolicy: Always postgresDeploymentSpec: imagePullPolicy: Always instances: 2 # we set a minimum of 2 pods for the HA storageClassName: gp2&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cat kustomization.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is the Kustomization file that contains all files we will use for this installation.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization commonAnnotations: argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true # To skip the dry run for missing resource types, use the following annotation resources: - 00_namespace_rhsso-dev.yaml - 01_rhsso-operator_resourcegroups.yaml - 02_rhsso-operator.yaml - 03_deploy_rhsso-dev.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now that we understand the files that we are going to install, we have to take a look at the Argo CD application that is going to deploy and sync them:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd../../bootstrap/deploy/application/01_rhsso-dev $ cat 01_rh-sso.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here we can take a look at a regular application for Argo CD; we are going to deploy it in the same namespace as Argo CD and aim it at our Git repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: annotations: argocd.argoproj.io/sync-wave: "0" name: rhsso-dev namespace: openshift-gitops spec: destination: name: '' namespace: openshift-gitops server: 'https://kubernetes.default.svc' source: path: resources/01_rhsso-dev # we specify the folder for the files repoURL: 'https://github.com/ignaciolago/keycloak-gitops.git' # the repository url targetRevision: HEAD # and branch, in this case HEAD / main project: default syncPolicy: automated: prune: true selfHeal: true syncOptions: - PruneLast=true&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The kustomize file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cat kustomization.yaml &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization commonAnnotations: argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true resources: - 01_rh-sso.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now we are going to apply all these files to start the installation.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -k bootstrap/deploy/application/01_rhsso-dev&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;application.argoproj.io/rhsso-dev created&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We have to wait for all pods to be in a running state.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ watch oc get pods -n rhsso-dev&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;NAME READY STATUS RESTARTS AGE keycloak-0 0/1 Init:0/1 0 4s keycloak-postgresql-59f5b79f4b-bbck4 0/1 Pending 0 4s rhsso-operator-7d8f749748-hc888 1/1 Running 0 6m56s&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;NAME READY STATUS RESTARTS AGE keycloak-0 1/1 Running 0 4m41s keycloak-1 1/1 Running 0 3m40s keycloak-postgresql-59f5b79f4b-bbck4 1/1 Running 0 5m30s rhsso-operator-7d8f749748-hc888 1/1 Running 0 7m20s&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can track the process and status updates of the components, as shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image3_6.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image3_6.png?itok=dQm4QRLi" width="600" height="293" alt="Observing the status and installation process in Argo CD." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: Track the installation status updates of the components on this Argo CD status page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;When they are all in sync, the status page will appear as in Figure 3.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image7_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image7_0.png?itok=WJD1OuZZ" width="600" height="466" alt="all in Sync Status" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: Status page shows that all processes are in sync.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once the installation is ready, we have to recover the single sign-on route as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;@oc get route keycloak -n rhsso-dev --template='https://{{.spec.host}}' https://keycloak-rhsso-dev.apps.cluster-lvn9g.lvn9g.sandbox1571.opentlc.com&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can access the GUI by entering this URL in the browser to verify that it is working (Figure 4).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image8.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image8.png?itok=yAbM1wyD" width="1391" height="578" alt="Red Hat single sign-on (SSO) login page credentials" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: The SSO login page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;We need the credentials in order to gain access. To recover the credentials for this, we must either use the &lt;code&gt;oc&lt;/code&gt; CLI or the OpenShift GUI to get to the secret and decode it. The secret is called &lt;code&gt;credential-rhsso-dev&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; The second part of the name will depend on the name of the instance.&lt;/p&gt; &lt;h3&gt;Recover the credentials from the GUI&lt;/h3&gt; &lt;p&gt;Figure 5 shows the credentials secret details on OpenShift.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image22.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image22.png?itok=y1vZZjQF" width="1440" height="974" alt="Credentials Secret on Openshift" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: The credentials secret details.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Recover the credentials from the oc CLI&lt;/h3&gt; &lt;p&gt;We can recover the credentials from the oc CLI as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get secret credential-rhsso-dev -n rhsso-dev -o jsonpath="{.data['ADMIN_PASSWORD']}" | base64 -d&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;n3XViKMKmg_ZFw==&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We will use the credentials to authenticate. After logging in, we can check that everything is working (Figure 6).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image11_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image11_0.png?itok=Jpum3J18" width="1440" height="540" alt="SSO Already Login" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 6: SSO Master details.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Congratulations, you have successfully deployed a single sign-on using Argo CD!&lt;/p&gt; &lt;h3&gt;Deploy a single sign-on for the product environment&lt;/h3&gt; &lt;p&gt;We cannot use a namespace-contained deployment for production because the database would be running inside a pod—this would be a Single Point Of Failure (SPoF) and we can not allow it! So we will learn how to deploy using an external (and hopefully highly available) database.&lt;/p&gt; &lt;p&gt;First we need to add the following lines to our Keycloak resource like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: keycloak.org/v1alpha1 kind: Keycloak Metadata: {} spec: multiAvailablityZones: enabled: true externalDatabase: ## ADD THIS LINE enabled: true ## ADD THIS LINE &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Take a look at the resources folder and see the files for a prod deploy.&lt;/p&gt; &lt;p&gt;The namespace, operator group, and operator subscription are the same as before, and we can check it by doing the &lt;code&gt;cat&lt;/code&gt; again as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd../../../../resources/02_rhsso-prod $cat 00_namespace_rhsso-prod.yaml 01_rhsso-operator_resourcegroups.yaml 02_rh-sso-operator.yaml&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: v1 kind: Namespace metadata: annotations: argocd.argoproj.io/sync-wave: "0" name: rhsso-prod spec: {} --- apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: annotations: argocd.argoproj.io/sync-wave: "1" name: rhsso-prod namespace: rhsso-prod spec: targetNamespaces: - rhsso-prod --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: annotations: argocd.argoproj.io/sync-wave: "2" name: rhsso-operator namespace: rhsso-prod spec: channel: stable installPlanApproval: Automatic name: rhsso-operator source: redhat-operators sourceNamespace: openshift-marketplace startingCSV: rhsso-operator.7.6.1-opr-005&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But we have one new file &lt;code&gt;01_secret_rhsso-prod-database.yaml&lt;/code&gt;. This file is a secret containing the credentials for the external database.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cat 01_secret_rhsso-prod-database.yaml &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: v1 kind: Secret metadata: annotations: argocd.argoproj.io/sync-wave: "1" name: keycloak-db-secret namespace: rhsso-prod stringData: POSTGRES_DATABASE: "pgsql-rhsso-prod" POSTGRES_USERNAME: "pgsql-admin" POSTGRES_PASSWORD: "pgsql-password" POSTGRES_EXTERNAL_ADDRESS: "pgsql-database-url" POSTGRES_EXTERNAL_PORT: "5432" # of course we have to change the values to real ones&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And the new lines in our &lt;code&gt;03_deploy_rhsso-prod.yaml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cat 03_deploy_rhsso-prod.yaml &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: keycloak.org/v1alpha1 kind: Keycloak metadata: annotations: argocd.argoproj.io/sync-wave: "3" name: rhsso-prod labels: app: rhsso-prod namespace: rhsso-prod spec: multiAvailablityZones: enabled: true externalDatabase: # this one enabled: true #this one keycloakDeploymentSpec: imagePullPolicy: Always postgresDeploymentSpec: imagePullPolicy: Always instances: 2 storageClassName: gp2&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can get the logs for one of the pods and check if it is running by using the command line or the GUI. We can see in the following log that it is up.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[0m[0m22:46:33,951 INFO [org.hibernate.annotations.common.Version] (ServerService Thread Pool -- 82) HCANN000001: Hibernate Commons Annotations {5.0.5.Final-redhat-00002} [0m[0m22:46:34,070 INFO [org.hibernate.dialect.Dialect] (ServerService Thread Pool -- 82) HHH000400: Using dialect: org.hibernate.dialect.PostgreSQL95Dialect [0m[0m22:46:34,099 INFO [org.hibernate.engine.jdbc.env.internal.LobCreatorBuilderImpl] (ServerService Thread Pool -- 82) HHH000424: Disabling contextual LOB creation as createClob() method threw error : java.lang.reflect.InvocationTargetException [0m[0m22:46:34,102 INFO [org.hibernate.type.BasicTypeRegistry] (ServerService Thread Pool -- 82) HHH000270: Type registration [java.util.UUID] overrides previous : org.hibernate.type.UUIDBinaryType@33731349 [0m[0m22:46:34,106 INFO [org.hibernate.envers.boot.internal.EnversServiceImpl] (ServerService Thread Pool -- 82) Envers integration enabled? : true [0m[0m22:46:34,301 INFO [org.hibernate.orm.beans] (ServerService Thread Pool -- 82) HHH10005002: No explicit CDI BeanManager reference was passed to Hibernate, but CDI is available on the Hibernate ClassLoader. [0m[0m22:46:34,890 INFO [org.hibernate.validator.internal.util.Version] (ServerService Thread Pool -- 82) HV000001: Hibernate Validator 6.0.23.Final-redhat-00001 [0m[0m22:46:35,634 INFO [org.hibernate.hql.internal.QueryTranslatorFactoryInitiator] (ServerService Thread Pool -- 82) HHH000397: Using ASTQueryTranslatorFactory [0m[0m22:46:36,014 INFO [org.keycloak.services] (ServerService Thread Pool -- 82) KC-SERVICES0050: Initializing master realm [0m[0m22:46:36,744 INFO [org.keycloak.services] (ServerService Thread Pool -- 82) KC-SERVICES0006: Importing users from '/opt/eap/standalone/configuration/keycloak-add-user.json' [0m[0m22:46:37,010 INFO [org.keycloak.services] (ServerService Thread Pool -- 82) KC-SERVICES0009: Added user 'admin' to realm 'master'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Even if we try to change or delete any of the components like the single sign-on, the operator, or the namespace, Argo CD is going to redeploy according to the info on our Git repository.&lt;/p&gt; &lt;h2&gt;Deploying single sign-on is complete&lt;/h2&gt; &lt;p&gt;We have demonstrated how we can leverage Argo CD to not only deploy and define the state of our single sign-on instance, but also manage the state without any external interference or input. This can be done in a dev and production environment using a GitOps approach.&lt;/p&gt; &lt;p&gt;If you have questions, comment below. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/10/how-deploy-single-sign-code-using-gitops" title="How to deploy single sign-on as code using GitOps"&gt;How to deploy single sign-on as code using GitOps&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Pablo Castelo, Ignacio Lago</dc:creator><dc:date>2023-04-10T07:00:00Z</dc:date></entry><entry><title>JBoss Tools 4.27.0.Final for Eclipse 2023-03</title><link rel="alternate" type="text/html" href="https://tools.jboss.org/blog/4.27.0.final.html" /><category term="release" /><category term="jbosstools" /><category term="jbosscentral" /><author><name>sbouchet</name></author><id>https://tools.jboss.org/blog/4.27.0.final.html</id><updated>2023-04-07T10:03:20Z</updated><published>2023-04-07T00:00:00Z</published><content type="html">&lt;div&gt;&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Happy to announce 4.27.0.Final build for Eclipse 2023-03.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Downloads available at &lt;a href="https://tools.jboss.org/downloads/jbosstools/2023-03/4.27.0.Final.html"&gt;JBoss Tools 4.27.0 Final&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="what-is-new"&gt;&lt;a class="anchor" href="#what-is-new"&gt;&lt;/a&gt;What is New?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Full info is at &lt;a href="https://tools.jboss.org/documentation/whatsnew/jbosstools/4.27.0.Final.html"&gt;this page&lt;/a&gt;. Some highlights are below.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="general"&gt;&lt;a class="anchor" href="#general"&gt;&lt;/a&gt;General&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We made several fixes in our server and javaee component to support jakarta namespaces.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="hibernate-tools"&gt;&lt;a class="anchor" href="#hibernate-tools"&gt;&lt;/a&gt;Hibernate Tools&lt;/h3&gt; &lt;div class="sect3"&gt; &lt;h4 id="runtime-provider-updates"&gt;&lt;a class="anchor" href="#runtime-provider-updates"&gt;&lt;/a&gt;Runtime Provider Updates&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The Hibernate 6.2 runtime provider now incorporates Hibernate Core version 6.2.0.CR4, Hibernate Ant version 6.2.0.CR4 and Hibernate Tools version 6.2.0.CR4.Final.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="and-more"&gt;&lt;a class="anchor" href="#and-more"&gt;&lt;/a&gt;And more…​&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You can find more noteworthy updates in on &lt;a href="https://tools.jboss.org/documentation/whatsnew/jbosstools/4.27.0.Final.html"&gt;this page&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Stéphane Bouchet&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;</content><summary>Happy to announce 4.27.0.Final build for Eclipse 2023-03. Downloads available at JBoss Tools 4.27.0 Final. What is New? Full info is at this page. Some highlights are below. General We made several fixes in our server and javaee component to support jakarta namespaces. Hibernate Tools Runtime Provider Updates The Hibernate 6.2 runtime provider now incorporates Hibernate Core version 6.2.0.CR4, Hibernate Ant version 6.2.0.CR4 and Hibernate Tools version 6.2.0.CR4.Final. And more…​ You can find more noteworthy updates in on this page. Enjoy! Stéphane Bouchet ...</summary><dc:creator>sbouchet</dc:creator><dc:date>2023-04-07T00:00:00Z</dc:date></entry><entry><title>Tips for handling localized ranges in regular expressions</title><link rel="alternate" href="https://developers.redhat.com/blog/2020/12/22/what-to-do-about-rational-ranges-in-regular-expressions" /><author><name>Carlos O'Donell</name></author><id>6aac5134-349d-44f1-ba68-1d84eab780e0</id><updated>2023-04-06T07:00:00Z</updated><published>2023-04-06T07:00:00Z</published><summary type="html">&lt;p&gt;Developers as well as casual &lt;code&gt;grep&lt;/code&gt; users are accustomed to using ranges in &lt;a href="https://developers.redhat.com/articles/2022/09/14/beginners-guide-regular-expressions-grep"&gt;regular expressions&lt;/a&gt;, such as &lt;code&gt;[a-zA-Z]&lt;/code&gt; or &lt;code&gt;[0-9]&lt;/code&gt;. However, they often don't realize that these regular expressions harbor problems that can lead to unexpected behavior.&lt;/p&gt; &lt;p&gt;This article delves into the issues with using ranges in different locales and the solutions sought by developers of various libraries, including the GNU &lt;a href="https://developers.redhat.com/topics/c"&gt;C&lt;/a&gt; Library (glibc).&lt;/p&gt; &lt;h2&gt;The problem with regular expression ranges&lt;/h2&gt; &lt;p&gt;Under the &lt;a href="https://pubs.opengroup.org/onlinepubs/9699919799/xrat/V4_xbd_chap09.html"&gt;POSIX standard&lt;/a&gt;, a regular expression using a range expression has unspecified behavior in any locale other than the POSIX locale. This locale applies only to programs or environments whose environment variables for the locale (such as &lt;code&gt;LANG&lt;/code&gt; or &lt;code&gt;LC_ALL&lt;/code&gt;) specify either &lt;code&gt;POSIX&lt;/code&gt; or &lt;code&gt;C&lt;/code&gt;, or who don't have those environment variables set at all.&lt;/p&gt; &lt;p&gt;Of course, this hardly ever happens. Most people specify their country and language when setting up their system and get a locale such as &lt;code&gt;en_US.UTF-8&lt;/code&gt;, in this case, indicating U.S. English with UTF-8 characters.&lt;/p&gt; &lt;p&gt;For most programs and users, therefore, a popular regular expression range such as &lt;code&gt;[a-zA-Z]&lt;/code&gt; or &lt;code&gt;[0-9]&lt;/code&gt; has undefined and ultimately unreliable behavior. In theory, users should employ bracket expressions such as &lt;code&gt;[[:alpha:]]&lt;/code&gt; and &lt;code&gt;[[:digit:]]&lt;/code&gt;. In practice, it works as expected in many, but not all, locales.&lt;/p&gt; &lt;p&gt;What should a library do to support developers and make developing applications easier? We will explore current and upcoming solutions in the next sections.&lt;/p&gt; &lt;h2&gt;Possible solutions and their relationships to POSIX and Unicode&lt;/h2&gt; &lt;p&gt;There are a number of possible solutions. The support for ranges such as &lt;code&gt;[a-zA-Z]&lt;/code&gt; in the C (POSIX) locale is a clue that support for the ranges was implemented by early C libraries when ASCII was the norm. Although there are many conflicting solutions, each generally maps to one of the following implementations:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Native character order (NCO):&lt;/strong&gt; This means that a developer looking at a code chart for the character set can logically identify all characters in the range by reviewing, in order, those characters in the code chart from the start of the range to the end of the range.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Collation element order (CEO):&lt;/strong&gt; This means that a developer looking at the locale sources for the current locale can logically identify all characters in the range by reviewing, in order, those characters in the &lt;a href="https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap07.html"&gt;&lt;code&gt;LC_COLLATE&lt;/code&gt;&lt;/a&gt; definition in the POSIX locale sources (later compiled into the binary locale on your system, e.g., &lt;code&gt;en_US.UTF-8&lt;/code&gt;) from the start of the range to the end of the range.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Collation sequence order (CSO):&lt;/strong&gt; This means that a developer looking at, for one definition of a natural language order, a dictionary with said natural language order can logically identify all characters in the range by reviewing, in order, those characters in the dictionary from the start of the range to the end of the range.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;For example, in the article &lt;a href="https://www.boost.org/doc/libs/1_38_0/libs/regex/doc/html/boost_regex/syntax/basic_extended.html"&gt;Boost C++ POSIX regular extended expression APIs&lt;/a&gt;, the authors implemented CSO with an &lt;a href="https://www.boost.org/doc/libs/1_38_0/libs/regex/doc/html/boost_regex/ref/syntax_option_type/syntax_option_type_extended.html"&gt;option&lt;/a&gt; to fall back to NCO. As another example, the &lt;a href="https://www.gnu.org/software/gawk/manual/html_node/Regexp.html"&gt;GNU Awk (Gawk) implementation&lt;/a&gt; has two modes: a "traditional" mode that emulates NCO within certain ASCII ranges and a POSIX-based mode that emulates CSO. The Boost and Gawk implementations offer a very similar degree of choice between NCO and CSO.&lt;/p&gt; &lt;p&gt;In glibc, the implementation is based on the early POSIX specifications that required CEO. In the built-in C and POSIX locale, the NCO and CEO are equivalent because the ASCII character set order can be ordered the same as the collation elements in the locale source specification. The glibc locale for &lt;code&gt;en_US.UTF-8&lt;/code&gt; makes the NCO and CEO equivalent for lowercase Latin characters, uppercase Latin characters, and numbers in order to preserve developer expectations for sorting these ranges; e.g., lowercase Latin characters are not interleaved with uppercase Latin characters.&lt;/p&gt; &lt;p&gt;CEO and CSO require large element lists and thus add a lot more overhead to implementations than NCO.&lt;/p&gt; &lt;p&gt;The published &lt;a href="https://www.iso.org/standard/68309.html"&gt;ISO 14651 (2020) standard&lt;/a&gt;, most recently derived from Unicode 13.0.0 (2020), defines the international string ordering and comparison, and glibc uses this standard as the basis for string collation. The &lt;a href="https://unicode-org.github.io/icu/userguide/collation/"&gt;collation element ordering&lt;/a&gt; in the ISO standard interleaves lowercase and uppercase characters in such a way that CEO is more aligned to logical groups of letters e.g. A and a, instead of NCO. Direct usage of ISO 14651 in glibc &lt;a href="https://sourceware.org/bugzilla/show_bug.cgi?id=23393"&gt; caused regressions&lt;/a&gt; due to this grouping; e.g., &lt;code&gt;[a-z]&lt;/code&gt; would match &lt;code&gt;A&lt;/code&gt; unexpectedly.&lt;/p&gt; &lt;h2&gt;Current and upcoming solutions&lt;/h2&gt; &lt;p&gt;Boost's interface allows one to choose between a logical NCO or CSO (as defined for a single natural language ordering), thus offering two of three solutions listed in the previous section. A user who desires a distinct CEO can create a completely new locale source definition and distribute that to users that want a distinct ordering. Thread-safe locale APIs can be used to set and use the locale on a per-thread basis.&lt;/p&gt; &lt;p&gt;The APIs implemented by the ICU project support many possible CSOs for a given language, including dictionary sort, address book sort, calendar sort, etc. No single CSO will solve the needs of all users.&lt;/p&gt; &lt;p&gt;The glibc implementation of CEO does not meet the needs of developers who are either looking at a code chart or applying common-sense logic to natural language ordering. Migrating glibc from CEO to CSO seems like a logical way forward, but the internal implementation will need to be significantly improved to support this transition. The most straightforward first step is a &lt;a href="https://sourceware.org/bugzilla/show_bug.cgi?id=17318"&gt;C.UTF-8 that uses NCO in glibc&lt;/a&gt; and avoids the overhead of CEO or CSO.&lt;/p&gt; &lt;p&gt;With the release of glibc 2.35 in February 2022, the project now has an official harmonized and C.UTF-8 that will use NCO for ASCII regular expression ranges and NCO for collation (code-point collation order).&lt;/p&gt; &lt;p&gt;You can already use this new C.UTF-8 locale in Fedora (starting with Fedora 35).  In the future, C.UTF-8 will be &lt;a href="https://sourceware.org/bugzilla/show_bug.cgi?id=28255"&gt;extended to allow rational ranges that cover all code points in NCO&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/blog/2020/12/22/what-to-do-about-rational-ranges-in-regular-expressions" title="Tips for handling localized ranges in regular expressions"&gt;Tips for handling localized ranges in regular expressions&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Carlos O'Donell</dc:creator><dc:date>2023-04-06T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - April, 06 2023</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2023-04-06.html" /><category term="quarkus" /><category term="vertx" /><category term="java" /><category term="wildfly" /><category term="keycloak" /><author><name>Don Naro</name><uri>https://www.jboss.org/people/don-naro</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2023-04-06.html</id><updated>2023-04-06T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, vertx, java, wildfly, keycloak"&gt; &lt;h1&gt;This Week in JBoss - April, 06 2023&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hi, how are you? Welcome back to another edition of the JBoss Editorial with exciting news and updates from your JBoss communities.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Here are the most recent releases for this edition:&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-3-0-0-cr1-released/"&gt;Quarkus 3.0.0 CR1&lt;/a&gt; and &lt;a href="https://quarkus.io/blog/quarkus-3-0-0-cr2-released/"&gt;Quarkus 3.0.0 CR2&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-4-4-1/"&gt;Eclipse Vert.x 4.4.1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2023/03/keycloak-2102-released"&gt;Keycloak 21.0.2&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org//news/2023/03/30/WildFly28-Beta-Released/"&gt;WildFly 28 Beta1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_debugging_restassured_tests"&gt;Debugging RestAssured Tests&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://www.mastertheboss.com/various-stuff/testing-java/debugging-restassured-tests/"&gt;Debugging RestAssured Tests&lt;/a&gt;, by Francesco Marchioni&lt;/p&gt; &lt;p&gt;Anyone running REST Assured tests has probably run into errors such as failing assertions or incorrect responses. If that is the case for you, then this article from Francesco will no doubt be valuable as he explains some solid techniques for debugging that quickly identify root causes and save you time getting things fixed.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_extending_the_life_of_keycloak_adapters"&gt;Extending the life of Keycloak adapters&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2023/03/adapter-deprecation-update"&gt;Update on deprecation of Keycloak adapters&lt;/a&gt;, by Stian Thorgersen&lt;/p&gt; &lt;p&gt;Following up on the announcement to deprecate Keycloak adapters, Stian has provided an udpate on what the future holds for the adapters.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_cross_resource_sharing_on_wildfly"&gt;Cross resource sharing on WildFly&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/web/jboss-web-server/how-to-configure-cors-on-wildfly/"&gt;How to configure CORS on WildFly&lt;/a&gt;, by Francesco Marchioni&lt;/p&gt; &lt;p&gt;Francesco delivers another insightful tutorial that quickly gets WildFly set up for cross-domain requests.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_video_corner"&gt;Video corner&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Check out some recent awesome YouTube videos from the community.&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/i8nHKmGxGuM"&gt;Getting Started with Accessing Your Application Portfolio&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/live/82NjJ7gDzv0"&gt;Quarkus Insights #124: 1000 ways to deploy Quarkus&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/live/cU-2witNthQ"&gt;Quarkus Insights #123: 10 things I like about Quarkus&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/3zzkGiVW7p8"&gt;Getting started with Ansible for Application Services&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/HKcrtQuWimY"&gt;Boosting Engineering Efficiency with OpenTelemetry, Keptn &amp;#38; Tyk&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;p&gt;&lt;em&gt;That’s all folks! Please join us again in two weeks for another round of our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/don-naro.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Don Naro&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Don Naro</dc:creator></entry><entry><title>Kubernetes Patterns: The path to cloud native</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/05/kubernetes-patterns-path-cloud-native" /><author><name>Bilgin Ibryam, Roland Huß</name></author><id>0efdb125-e706-4dac-8b88-8f29ededc4fd</id><updated>2023-04-05T07:00:00Z</updated><published>2023-04-05T07:00:00Z</published><summary type="html">&lt;p class="Indent1"&gt;&lt;strong&gt;Note: &lt;/strong&gt;The following is an excerpt from &lt;strong&gt;&lt;a href="https://developers.redhat.com/e-books/kubernetes-patterns-2nd-edition"&gt;Kubernetes Patterns, Second Edition&lt;/a&gt;&lt;/strong&gt;&lt;em&gt; &lt;/em&gt;by Bilgin Ibryam and Roland Huß (O'Reilly Media, March 2023). Download the e-book to learn how to solve common cloud native challenges with proven design patterns.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/microservices/"&gt;Microservices&lt;/a&gt; is among the most popular architectural styles for creating cloud native applications. They tackle software complexity through modularization of business capabilities and trading development complexity for operational complexity. That is why a key prerequisite for becoming successful with microservices is to create applications that can be operated at scale through &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;As part of the microservices movement, there is a tremendous amount of theory, techniques, and supplemental tools for creating microservices from scratch or for splitting monoliths into microservices. Most of these practices are based on &lt;a href="https://www.dddcommunity.org/book/evans_2003/"&gt;Domain-Driven Design&lt;/a&gt; by Eric Evans (Addison-Wesley) and the concepts of bounded contexts and aggregates. Bounded contexts deal with large models by dividing them into different components, and aggregates help to further group bounded contexts into modules with defined transaction boundaries. However, in addition to these business domain considerations, for each distributed system—whether it is based on microservices or not—there are also technical concerns around its external structure, and runtime coupling. &lt;a href="https://developers.redhat.com/topics/containers"&gt;Containers&lt;/a&gt; and container orchestrators such as Kubernetes bring in new primitives and abstractions to address the concerns of distributed applications, and here we discuss the various options to consider when putting a distributed system into Kubernetes.&lt;/p&gt; &lt;p&gt;Throughout this book, we look at container and platform interactions by treating the containers as black boxes. However, we created this section to emphasize the importance of what goes into containers. Containers and cloud native platforms bring tremendous benefits to your distributed applications, but if all you put into containers is rubbish, you will get distributed rubbish at scale. Figure 1-1 shows the mixture of the skills required for creating good cloud native applications and where Kubernetes patterns fit in.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-04-04_at_9.50.52_pm.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-04-04_at_9.50.52_pm.png?itok=EohnmTT_" width="600" height="591" alt="High-level text diagram depicting skills required for creating good cloud native applications, with clean code at the lowest level." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The path to cloud native. Used with permission from Kubernetes Patterns, Second Edition by Bilgin Ibryam and Roland Huß (O’Reilly). Copyright 2023 Bilgin Ibryam and Roland Huß.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;At a high level, creating good cloud native applications requires familiarity with multiple design techniques:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;At the lowest code level, every variable you define, every method you create, and every class you decide to instantiate plays a role in the long-term maintenance of the application. No matter what container technology and orchestration platform you use, the development team and the artifacts they create will have the most impact. It is important to grow developers who strive to write clean code, have the right number of automated tests, constantly refactor to improve code quality, and are guided by Software Craftsmanship principles at heart.&lt;/li&gt; &lt;li aria-level="1"&gt;Domain-driven design is about approaching software design from a business perspective with the intention of keeping the architecture as close to the real world as possible. This approach works best for object-oriented programming languages, but there are also other good ways to model and design software for real-world problems. A model with the right business and transaction boundaries, easy-to-consume interfaces, and rich APIs is the foundation for successful containerization and automation later.&lt;/li&gt; &lt;li aria-level="1"&gt;The hexagonal architecture and its variations, such as Onion and Clean architectures, improve the flexibility and maintainability of applications by decoupling the application components and providing standardized interfaces for interacting with them. By decoupling the core business logic of a system from the surrounding infrastructure, hexagonal architecture makes it easier to port the system to different environments or platforms. These architectures complement domain-driven design and help arrange application code with distinct boundaries and externalized infrastructure dependencies.&lt;/li&gt; &lt;li aria-level="1"&gt;The microservices architectural style and the twelve-factor app methodology very quickly evolved to become the norm for creating distributed applications and they provide valuable principles and practices for designing changing distributed applications. Applying these principles lets you create implementations that are optimized for scale, resiliency, and pace of change, which are common requirements for any modern software today.&lt;/li&gt; &lt;li aria-level="1"&gt;Containers were very quickly adopted as the standard way of packaging and running distributed applications, whether these are microservices or functions. Creating modular, reusable containers that are good cloud native citizens is another fundamental prerequisite. Cloud native is a term used to describe principles, patterns, and tools to automate containerized applications at scale. We use cloud native interchangeably with Kubernetes, which is the most popular open source cloud native platform available today.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In this book, we are not covering clean code, domain-driven design, hexagonal architecture, or microservices. We are focusing only on the patterns and practices addressing the concerns of the container orchestration. But for these patterns to be effective, your application needs to be designed well from the inside by using clean code practices, domain-driven design, hexagonal architecture-like isolation of external dependencies, microservices principles, and other relevant design techniques.&lt;/p&gt; &lt;h2&gt;Distributed primitives&lt;/h2&gt; &lt;p&gt;To explain what we mean by new abstractions and primitives, here we compare them with the well-known object-oriented programming (OOP), and Java specifically. In the OOP universe, we have concepts such as class, object, package, inheritance, encapsulation, and polymorphism. Then the Java runtime provides specific features and guarantees on how it manages the lifecycle of our objects and the application as a whole.&lt;/p&gt; &lt;p&gt;The Java language and the Java Virtual Machine (JVM) provide local, in-process building blocks for creating applications. Kubernetes adds an entirely new dimension to this well-known mindset by offering a new set of distributed primitives and runtime for building distributed systems that spread across multiple nodes and processes. With Kubernetes at hand, we don’t rely only on the local primitives to implement the whole application behavior.&lt;/p&gt; &lt;p&gt;We still need to use the object-oriented building blocks to create the components of the distributed application, but we can also use Kubernetes primitives for some of the application behaviors. Table 1-1 shows how various development concepts are realized differently with local and distributed primitives in the JVM and Kubernetes, respectively.&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="731"&gt;&lt;caption&gt;Table 1: Local and distributed primitives.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th scope="col"&gt;Concept&lt;/th&gt; &lt;th scope="col"&gt;Local primitives&lt;/th&gt; &lt;th scope="col"&gt;Distributed primitive&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Behavior encapsulation &lt;/td&gt; &lt;td&gt;Class&lt;/td&gt; &lt;td&gt;Container image&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Behavior instance &lt;/td&gt; &lt;td&gt;Object&lt;/td&gt; &lt;td&gt;Container&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Unit of reuse &lt;/td&gt; &lt;td&gt;.jar&lt;/td&gt; &lt;td&gt;Container image&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Composition&lt;/td&gt; &lt;td&gt;Class A contains Class B&lt;/td&gt; &lt;td&gt;Sidecar pattern&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Inheritance &lt;/td&gt; &lt;td&gt;Class A extends Class B&lt;/td&gt; &lt;td&gt;A container’s FROM parent&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Deployment unit &lt;/td&gt; &lt;td&gt;.jar/.war/.ear&lt;/td&gt; &lt;td&gt;image&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Buildtime/Runtime isolation &lt;/td&gt; &lt;td&gt;Module, package, class&lt;/td&gt; &lt;td&gt;Pod&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Initialization preconditions &lt;/td&gt; &lt;td&gt;Constructor&lt;/td&gt; &lt;td&gt;Namespace, Pod, container&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Postinitialization trigger &lt;/td&gt; &lt;td&gt;Init-method&lt;/td&gt; &lt;td&gt;postStart&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Predestroy trigger &lt;/td&gt; &lt;td&gt;Destroy-method&lt;/td&gt; &lt;td&gt;preStop&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Cleanup procedure &lt;/td&gt; &lt;td&gt;finalize(), shutdown hook&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Asynchronous and parallel execution &lt;/td&gt; &lt;td&gt;ThreadPoolExecutor, ForkJoinPool&lt;/td&gt; &lt;td&gt;Job&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Periodic task &lt;/td&gt; &lt;td&gt;Timer, ScheduledExecutorService&lt;/td&gt; &lt;td&gt;CronJob&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Background task &lt;/td&gt; &lt;td&gt;Daemon thread&lt;/td&gt; &lt;td&gt;DaemonSet&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Configuration management&lt;/td&gt; &lt;td&gt;System.getenv(), Properties &lt;/td&gt; &lt;td&gt;ConfigMap, Secret&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The in-process primitives and the distributed primitives have commonalities, but they are not directly comparable and replaceable. They operate at different abstraction levels and have different preconditions and guarantees. Some primitives are supposed to be used together. For example, we still have to use classes to create objects and put them into container images. However, some other primitives such as CronJob in Kubernetes can completely replace the ExecutorService behavior in Java.&lt;/p&gt; &lt;p&gt;Next, let’s see a few distributed abstractions and primitives from Kubernetes that are especially interesting for application developers.&lt;/p&gt; &lt;h3&gt;Containers&lt;/h3&gt; &lt;p&gt;Containers are the building blocks for Kubernetes-based cloud native applications. If we make a comparison with OOP and Java, container images are like classes, and containers are like objects. The same way we can extend classes to reuse and alter behavior, we can have container images that extend other container images to reuse and alter behavior. The same way we can do object composition and use functionality, we can do container compositions by putting containers into a Pod and using collaborating containers.&lt;/p&gt; &lt;p&gt;If we continue the comparison, Kubernetes would be like the JVM but spread over multiple hosts, and it would be responsible for running and managing the containers.Init containers would be something like object constructors; DaemonSets would besimilar to daemon threads that run in the background (like the Java Garbage Collector,for example). A Pod would be something similar to an Inversion of Control (IoC) context (Spring Framework, for example), where multiple running objects share amanaged lifecycle and can access one another directly.&lt;/p&gt; &lt;p&gt;The parallel doesn’t go much further, but the point is that containers play a fundamental role in Kubernetes, and creating modularized, reusable, single-purpose container images is fundamental to the long-term success of any project and even the containers’ ecosystem as a whole. Apart from the technical characteristics of a container image that provide packaging and isolation, what does a container represent, and what is its purpose in the context of a distributed application? Here are a few suggestions on how to look at containers:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;A container image is the unit of functionality that addresses a single concern.&lt;/li&gt; &lt;li aria-level="1"&gt;A container image is owned by one team and has its own release cycle.&lt;/li&gt; &lt;li aria-level="1"&gt;A container image is self-contained and defines and carries its runtime dependencies.&lt;/li&gt; &lt;li aria-level="1"&gt;A container image is immutable, and once it is built, it does not change; it is configured.&lt;/li&gt; &lt;li aria-level="1"&gt;A container image defines its resource requirements and external dependencies.&lt;/li&gt; &lt;li aria-level="1"&gt;A container image has well-defined APIs to expose its functionality.&lt;/li&gt; &lt;li aria-level="1"&gt;A container typically runs as a single Unix process. &lt;/li&gt; &lt;li aria-level="1"&gt;A container is disposable and safe to scale up or down at any moment. In addition to all these characteristics, a proper container image is modular. It is parameterized and created for reuse in the different environments in which it is going to run. Having small, modular, and reusable container images leads to the creation of more specialized and stable container images in the long term, similar to a great reusable library in the programming language world.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Pods&lt;/h3&gt; &lt;p&gt;Looking at the characteristics of containers, we can see that they are a perfect match for implementing the microservices principles. A container image provides a single unit of functionality, belongs to a single team, has an independent release cycle, and provides deployment and runtime isolation. Most of the time, one microservice corresponds to one container image.&lt;/p&gt; &lt;p&gt;However, most cloud native platforms offer another primitive for managing the lifecycle of a group of containers—in Kubernetes, it is called a Pod. A Pod is an atomic unit of scheduling, deployment, and runtime isolation for a group of containers. All containers in a Pod are always scheduled to the same host, are deployed and scaled together, and can also share filesystem, networking, and process namespaces. This joint lifecycle allows the containers in a Pod to interact with one another over the filesystem or through networking via localhost or host interprocess communication mechanisms if desired (for performance reasons, for example). A Pod also represents a security boundary for an application. While it is possible to have containers with varying security parameters in the same Pod, typically all containers would have the same access level, network segmentation, and identity.&lt;/p&gt; &lt;p&gt;As you can see in Figure 1-2, at development and build time, a microservice corresponds to a container image that one team develops and releases. But at runtime, a microservice is represented by a Pod, which is the unit of deployment, placement, and scaling. The only way to run a container—whether for scale or migration—is through the Pod abstraction. Sometimes a Pod contains more than one container. In one such example, a containerized microservice uses a helper container at runtime, as Chapter 16, “Sidecar,” demonstrates.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-04-04_at_9.20.15_pm.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-04-04_at_9.20.15_pm.png?itok=oUkueEQq" width="600" height="384" alt="Diagram of a Pod managing two containers, one Java and one Python." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: A Pod as the deployment and management unit. Used with permission from Kubernetes Patterns, Second Edition by Bilgin Ibryam and Roland Huß (O’Reilly). Copyright 2023 Bilgin Ibryam and Roland Huß.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Containers, Pods, and their unique characteristics offer a new set of patterns and principles for designing microservices-based applications. We saw some of the characteristics of well-designed containers; now let’s look at some characteristics of a Pod:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;A Pod is the atomic unit of scheduling. That means the scheduler tries to find a host that satisfies the requirements of all containers that belong to the Pod (we cover some specifics around init containers in Chapter 15, “Init Container”). If you create a Pod with many containers, the scheduler needs to find a host that has enough resources to satisfy all container demands combined. This scheduling process is described in Chapter 6, “Automated Placement.”&lt;/li&gt; &lt;li aria-level="1"&gt;A Pod ensures colocation of containers. Thanks to the colocation, containers in the same Pod have additional means to interact with one another. The most common ways of communicating include using a shared local filesystem for exchanging data, using the localhost network interface, or using some host interprocess communication (IPC) mechanism for high-performance interactions.&lt;/li&gt; &lt;li aria-level="1"&gt;A Pod has an IP address, name, and port range that are shared by all containers belonging to it. That means containers in the same Pod have to be carefully configured to avoid port clashes, in the same way that parallel, running Unix processes have to take care when sharing the networking space on a host.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;A Pod is the atom of Kubernetes where your application lives, but you don’t access Pods directly—that is where Services enter the scene.&lt;/p&gt; &lt;h3&gt;Services&lt;/h3&gt; &lt;p&gt;Pods are ephemeral. They come and go at any time for all sorts of reasons (e.g., scaling up and down, failing container health checks, node migrations). A Pod IP address is known only after it is scheduled and started on a node. A Pod can be rescheduled to a different node if the existing node it is running on is no longer healthy. This means the Pod’s network address may change over the life of an application, and there is a need for another primitive for discovery and load balancing.&lt;/p&gt; &lt;p&gt;That’s where the Kubernetes Services come into play. The Service is another simple but powerful Kubernetes abstraction that binds the Service name to an IP address and port number permanently. So a Service represents a named entry point for accessing an application. In the most common scenario, the Service serves as the entry point for a set of Pods, but that might not always be the case. The Service is a generic primitive, and it may also point to functionality provided outside the Kubernetes cluster. As such, the Service primitive can be used for Service discovery and load balancing, and it allows altering implementations and scaling without affecting Service consumers. We explain Services in detail in Chapter 13, “Service Discovery.”&lt;/p&gt; &lt;h3&gt;Labels&lt;/h3&gt; &lt;p&gt;We have seen that a microservice is a container image at build time but is represented by a Pod at runtime. So what is an application that consists of multiple microservices? Here, Kubernetes offers two more primitives that can help you define the concept of an application: labels and namespaces.&lt;/p&gt; &lt;p&gt;Before microservices, an application corresponded to a single deployment unit with a single versioning scheme and release cycle. There was a single file for an application in a .war, .ear, or some other packaging format. But then, applications were split into microservices, which are independently developed, released, run, restarted, or scaled. With microservices, the notion of an application diminishes, and there are no key artifacts or activities that we have to perform at the application level. But if you still need a way to indicate that some independent services belong to an application, labels can be used. Let’s imagine that we have split one monolithic application into three microservices and another one into two microservices.&lt;/p&gt; &lt;p&gt;We now have five Pod definitions (and maybe many more Pod instances) that are independent of the development and runtime points of view. However, we may still need to indicate that the first three Pods represent an application and the other two Pods represent another application. Even the Pods may be independent, to provide a business value, but they may depend on one another. For example, one Pod may contain the containers responsible for the frontend, and the other two Pods are responsible for providing the backend functionality. If either of these Pods is down, the application is useless from a business point of view. Using label selectors gives us the ability to query and identify a set of Pods and manage it as one logical unit. Figure 1-3 shows how you can use labels to group the parts of a distributed application into specific subsystems.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-04-04_at_9.19.06_pm.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-04-04_at_9.19.06_pm.png?itok=ICEnOykA" width="600" height="331" alt="Diagram of two distributed applications grouped into specific subsystems via labels." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1-3. Labels used as an application identity for Pods. Used with permission from Kubernetes Patterns, Second Edition by Bilgin Ibryam and Roland Huß (O’Reilly). Copyright 2023 Bilgin Ibryam and Roland Huß.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Here are a few examples where labels can be useful:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Labels are used by ReplicaSets to keep some instances of a specific Pod running. That means every Pod definition needs to have a unique combination of labels used for scheduling.&lt;/li&gt; &lt;li aria-level="1"&gt;Labels are also heavily used by the scheduler. The scheduler uses labels for colocating or spreading Pods to the nodes that satisfy the Pods’ requirements.&lt;/li&gt; &lt;li aria-level="1"&gt;A label can indicate a logical grouping of a set of Pods and give an application identity to them.&lt;/li&gt; &lt;li aria-level="1"&gt;In addition to the preceding typical use cases, labels can be used to store metadata. It may be difficult to predict what a label could be used for, but it is best to have enough labels to describe all important aspects of the Pods. For example, having labels to indicate the logical group of an application, the business characteristics and criticality, the specific runtime platform dependencies such as hardware architecture, or location preferences are all useful.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Later, these labels can be used by the scheduler for more fine-grained scheduling, or the same labels can be used from the command line for managing the matching Pods at scale. However, you should not go overboard and add too many labels in advance.&lt;/p&gt; &lt;p&gt;You can always add them later if needed. Removing labels is much riskier as there is no straightforward way of finding out what a label is used for and what unintended effect such an action may cause.&lt;/p&gt; &lt;h4&gt;Annotations&lt;/h4&gt; &lt;p&gt;Another primitive very similar to labels is the annotation. Like labels, annotations are organized as a map, but they are intended for specifying nonsearchable metadata and for machine usage rather than human.&lt;/p&gt; &lt;p&gt;The information on the annotations is not intended for querying and matching objects. Instead, it is intended for attaching additional metadata to objects from various tools and libraries we want to use. Some examples of using annotations include build IDs, release IDs, image information, timestamps, Git branch names, pull request numbers, image hashes, registry addresses, author names, tooling information, and more. So while labels are used primarily for query matching and performing actions on the matching resources, annotations are used to attach metadata that can be consumed by a machine.&lt;/p&gt; &lt;h3&gt;Namespaces&lt;/h3&gt; &lt;p&gt;Another primitive that can also help manage a group of resources is the Kubernetes namespace. As we have described, a namespace may seem similar to a label, but in reality, it is a very different primitive with different characteristics and purposes.&lt;/p&gt; &lt;p&gt;Kubernetes namespaces allow you to divide a Kubernetes cluster (which is usually spread across multiple hosts) into a logical pool of resources. Namespaces provide scopes for Kubernetes resources and a mechanism to apply authorizations and other policies to a subsection of the cluster. The most common use case of namespaces is representing different software environments such as development, testing, integration testing, or production. Namespaces can also be used to achieve multitenancy and provide isolation for team workspaces, projects, and even specific applications. But ultimately, for a greater isolation of certain environments, namespaces are not enough, and having separate clusters is common. Typically, there is one nonproduction Kubernetes cluster used for some environments (development, testing, and integration testing) and another production Kubernetes cluster to represent performance testing and production environments.&lt;/p&gt; &lt;p&gt;Let’s look at some of the characteristics of namespaces and how they can help us in different scenarios:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;A namespace is managed as a Kubernetes resource.&lt;/li&gt; &lt;li aria-level="1"&gt;A namespace provides scope for resources such as containers, Pods, Services, or ReplicaSets. The names of resources need to be unique within a namespace but not across them.&lt;/li&gt; &lt;li aria-level="1"&gt;By default, namespaces provide scope for resources, but nothing isolates those resources and prevents access from one resource to another. For example, a Pod from a development namespace can access another Pod from a production namespace as long as the Pod IP address is known. “Network isolation across namespaces for creating a lightweight multitenancy solution is described in Chapter 24, “Network Segmentation.”&lt;/li&gt; &lt;li aria-level="1"&gt;Some other resources, such as namespaces, nodes, and PersistentVolumes, do not belong to namespaces and should have unique cluster-wide names.&lt;/li&gt; &lt;li aria-level="1"&gt;Each Kubernetes Service belongs to a namespace and gets a corresponding Domain Name Service (DNS) record that has the namespace in the form of &lt;code&gt;&lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local&lt;/code&gt;. So the namespace name is in the URL of every Service belonging to the given namespace. That’s one reason it is vital to name namespaces wisely.&lt;/li&gt; &lt;li aria-level="1"&gt;ResourceQuotas provide constraints that limit the aggregated resource consumption per namespace. With ResourceQuotas, a cluster administrator can control the number of objects per type that are allowed in a namespace. For example, a developer namespace may allow only five ConfigMaps, five Secrets, five Services, five ReplicaSets, five PersistentVolumeClaims, and ten Pods.&lt;/li&gt; &lt;li aria-level="1"&gt;ResourceQuotas can also limit the total sum of computing resources we can request in a given namespace. For example, in a cluster with a capacity of 32 GB RAM and 16 cores, it is possible to allocate 16 GB RAM and 8 cores for the production namespace, 8 GB RAM and 4 cores for the staging environment, 4 GB RAM and 2 cores for development, and the same amount for testing namespaces. The ability to impose resource constraints decoupled from the shape and the limits of the underlying infrastructure is invaluable.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Discussion&lt;/h2&gt; &lt;p&gt;We’ve only briefly covered a few of the main Kubernetes concepts we use in this book. However, there are more primitives used by developers on a day-by-day basis. For example, if you create a containerized service, there are plenty of Kubernetes abstractions you can use to reap all the benefits of Kubernetes. Keep in mind, these are only a few of the objects used by application developers to integrate a containerized service into Kubernetes. There are plenty of other concepts used primarily by cluster administrators for managing Kubernetes. Figure 1-4 gives an overview of the main Kubernetes resources that are useful for developers.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-04-04_at_9.14.50_pm.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-04-04_at_9.14.50_pm.png?itok=kOtFuemK" width="600" height="427" alt="Diagram listing Kubernetes concepts for developers" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: Kubernetes concepts for developers. Used with permission from Kubernetes Patterns, Second Edition by Bilgin Ibryam and Roland Huß (O’Reilly). Copyright 2023 Bilgin Ibryam and Roland Huß.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;With time, these new primitives give birth to new ways of solving problems, and some of these repetitive solutions become patterns. Throughout this book, rather than describing each Kubernetes resource in detail, we will focus on concepts that are proven as patterns.&lt;/p&gt; &lt;h2 id="download_gitops_cookbook-h2"&gt;Get the e-book: Kubernetes Patterns, 2nd Edition&lt;/h2&gt; &lt;p&gt;Kubernetes Patterns presents reusable patterns and principles for designing and implementing cloud native applications on Kubernetes. &lt;a href="https://developers.redhat.com/e-books/kubernetes-patterns-2nd-edition"&gt;&lt;strong&gt;Download the second edition e-book from Red Hat Developer.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/05/kubernetes-patterns-path-cloud-native" title="Kubernetes Patterns: The path to cloud native"&gt;Kubernetes Patterns: The path to cloud native&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bilgin Ibryam, Roland Huß</dc:creator><dc:date>2023-04-05T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 3.0.0.CR2 released</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-3-0-0-cr2-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-3-0-0-cr2-released/</id><updated>2023-04-05T00:00:00Z</updated><content type="html">Today, we released Quarkus 3.0.0.CR2, our last step before building the 3.0.0.Final bits. Please try it with your applications, the update is easy in a lot of cases, and report any problem to us by creating a GitHub issue. To upgrade your application to Quarkus 3.0, see the instructions below....</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title>Update and upgrade JBoss EAP with Ansible</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/04/update-and-upgrade-jboss-eap-ansible" /><author><name>Romain Pelisse</name></author><id>db3c5c1a-506e-450e-b437-eb0384a98609</id><updated>2023-04-04T07:00:00Z</updated><published>2023-04-04T07:00:00Z</published><summary type="html">&lt;p&gt;In this follow-up to &lt;a href="https://developers.redhat.com/articles/2022/02/08/automate-and-deploy-jboss-eap-cluster-ansible"&gt;Automate and deploy a JBoss EAP cluster with Ansible&lt;/a&gt;, we will explain how to maintain and keep those instances updated, again in a fully &lt;a href="https://developers.redhat.com/topics/automation"&gt;automated&lt;/a&gt; manner, leveraging Ansible and the &lt;a href="https://console.redhat.com/ansible/automation-hub/repo/published/redhat/eap/"&gt;Ansible collection for Red Hat JBoss Enterprise Application Platform (EAP)&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;Indeed, it is critical to ensure that all JEE application server instances always be up to date, especially in regard to security fixes. Therefore, we’ll discuss not only how to apply patches to update the server but also how to perform an upgrade to migrate to a new major version.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;In the &lt;a href="https://developers.redhat.com/articles/2022/02/08/automate-and-deploy-jboss-eap-cluster-ansible"&gt;previous article&lt;/a&gt;, we used the following playbook to install the WildFly cluster using Ansible on one single host. However, in this article, we’ll use only one instance of JBoss EAP (and no longer WildFly) for simplicity's sake.&lt;/p&gt; &lt;p&gt;You can use the following playbook to install an instance of JBoss EAP:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: Install EAP 7 hosts: all collections: - redhat.eap roles: - eap_install - eap_systemd&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; The playbook above uses the &lt;code&gt;redhat.eap&lt;/code&gt; certified collection instead of the upstream &lt;code&gt;middleware_automation.wildfly&lt;/code&gt; instance. The previous article used WildFly (the community version of the JEE application server) instead of JBoss EAP (the product supported by Red Hat). However, as updates are rarely produced for the upstream version (because of the fast release cycle, it’s easier to update to the next version), this article will focus on JBoss EAP, as product users often have to manage such updates.&lt;/p&gt; &lt;p&gt;For the playbook above to function as expected, you will need to install the collections &lt;code&gt;redhat.eap&lt;/code&gt; on the Ansible controller:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ansible-galaxy collection install redhat.eap Starting galaxy collection install process Process install dependency map Starting collection install process Downloading https://console.redhat.com/api/automation-hub/v3/plugin/ansible/content/published/collections/artifacts/redhat-eap-1.3.1.tar.gz to /root/.ansible/tmp/ansible-local-344ezcdnc0/tmpc0c7yq1u/redhat-eap-1.3.1-b5h7g9vf Downloading https://console.redhat.com/api/automation-hub/v3/plugin/ansible/content/published/collections/artifacts/redhat-redhat_csp_download-1.2.2.tar.gz to /root/.ansible/tmp/ansible-local-344ezcdnc0/tmpc0c7yq1u/redhat-redhat_csp_download-1.2.2-2kmr5p0m Installing 'redhat.eap:1.3.1' to '/root/.ansible/collections/ansible_collections/redhat/eap' redhat.eap:1.3.1 was installed successfully Installing 'redhat.redhat_csp_download:1.2.2' to '/root/.ansible/collections/ansible_collections/redhat/redhat_csp_download' redhat.redhat_csp_download:1.2.2 was installed successfully&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As &lt;code&gt;redhat.eap&lt;/code&gt; is a certified collection only available to Red Hat customers, you need to configure the &lt;code&gt;ansible.cfg&lt;/code&gt; file to use Ansible Automation Hub so that the collection can be retrieved:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# cat ansible.cfg [defaults] host_key_checking = False retry_files_enabled = False nocows = 1 [inventory] # fail more helpfully when the inventory file does not parse (Ansible 2.4+) unparsed_is_failed=true [galaxy] server_list = automation_hub, galaxy [galaxy_server.galaxy] url=https://galaxy.ansible.com/ [galaxy_server.automation_hub] url=https://cloud.redhat.com/api/automation-hub/ auth_url=https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token token=&lt;your-token&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;A bit of context first&lt;/h2&gt; &lt;p&gt;Before we continue, let's define what we mean (in this article) by update versus upgrade. &lt;/p&gt; &lt;h3&gt;Updates&lt;/h3&gt; &lt;p&gt;An &lt;strong&gt;update&lt;/strong&gt; to an existing JBoss EAP instance consists of the deployment of a series of fixes to the product’s code. Updates are provided to Red Hat customers through the &lt;a href="https://access.redhat.com/login?redirectTo=https%3A%2F%2Faccess.redhat.com"&gt;Red Hat Customer Portal&lt;/a&gt;. They contain a set of changes made against the JEE server code, either to fix an issue or address a security concern (or both). You need to use the JBoss CLI tool to deploy an update. However, as we’ll see soon, the &lt;code&gt;redhat.eap&lt;/code&gt; collection will take care of this for us.&lt;/p&gt; &lt;p&gt;It’s important to note that such an update only brings fixes to the server. No functionality changes (unless required to resolve a problem) nor API changes are performed. Therefore, an update does not change the major version of JBoss EAP—only the minor version. For instance, bringing JBoss EAP 7.4.0 to 7.4.6 (but not to 7.5).&lt;/p&gt; &lt;p&gt;Because an update to a more recent minor version only includes small changes, they rarely require modification to the applications hosted by JBoss EAP. They should be performed as quickly as possible in order to ensure the server can not be compromised by a known security issue.&lt;/p&gt; &lt;h3&gt;Upgrades&lt;/h3&gt; &lt;p&gt;An &lt;strong&gt;upgrade&lt;/strong&gt; is a more involved operation. Indeed, it comes with API changes and new features, meaning that, before being performed, the applications hosted by JBoss EAP should be tested against the new version and potentially adapted to work in this new context.&lt;/p&gt; &lt;p&gt;Also, an upgrade to JBoss EAP is a change of major version (EAP 7.3 to 7.4). This cannot be achieved by updating the files of the currently installed software. A complete, new installation needs to be performed, and the configuration, along with the hosted apps, needs to be migrated to this new root folder.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note: &lt;/strong&gt;This article focuses on the update and upgrade of the app server itself and, purposely, does not discuss configuration changes and application migration. On this front, too, the collection &lt;code&gt;redhat.eap&lt;/code&gt; provides some help by leveraging the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.4/html-single/using_the_jboss_server_migration_tool/index"&gt;JBoss migration tool&lt;/a&gt;. This scenario will be the topic of a follow-up article.&lt;/p&gt; &lt;h2&gt;Applying a cumulative patch&lt;/h2&gt; &lt;p&gt;Let’s consider the following scenario:&lt;/p&gt; &lt;p&gt;A series of JBoss EAP 7.3.0 instances were freshly installed in order to perform tests before production. The tests were successful, and the team now wants to promote those servers from testing to preprod. The main requirement (regarding JBoss EAP) is to run the latest version of the server (7.3.10).&lt;/p&gt; &lt;p&gt;Here is the playbook used to fully automate the installation process:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: "Update EAP to latest {{ eap_patch_version }}" hosts: eap_servers vars: eap_version: 7.3.0 eap_apply_cp: True eap_patch_version: 7.3.10 eap_instance_name: eap73 collections: - redhat.eap roles: - eap_install - eap_systemd&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This playbook relies entirely on the two roles provided by the &lt;code&gt;redhat.eap&lt;/code&gt; collection, to install EAP and start the associated service. The only information required is the server (major) version provided by the variable &lt;code&gt;eap_version&lt;/code&gt;. We also configured the collection to update this minor version (&lt;code&gt;eap_patch_apply&lt;/code&gt; set to &lt;code&gt;true&lt;/code&gt;) to the latest available minor version (7.3.10). We also added a variable to change the name of the service running the server to &lt;code&gt;eap73&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Once this playbook has run successfully and Ansible has started the server, we can see that we are now running the latest version available of JBoss EAP 7.3, as proven by the following line of the &lt;code&gt;server.log&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# tail -f /opt/jboss_eap/jboss-eap-7.3/standalone/log/server.log 2023-03-01 11:25:29,413 INFO [org.jboss.as.connector.subsystems.datasources] (MSC service thread 1-6) WFLYJCA0001: Bound data source [java:jboss/datasources/ExampleDS] 2023-03-01 11:25:29,483 INFO [org.jboss.as.patching] (MSC service thread 1-2) WFLYPAT0050: JBoss EAP cumulative patch ID is: jboss-eap-7.3.10.CP, one-off patches include: none 2023-03-01 11:25:29,494 WARN [org.jboss.as.domain.management.security] (MSC service thread 1-2) WFLYDM0111: Keystore /opt/jboss_eap/jboss-eap-7.3/standalone/configuration/application.keystore not found, it will be auto generated on first use with a self signed certificate for host localhost 2023-03-01 11:25:29,499 INFO [org.jboss.as.server.deployment.scanner] (MSC service thread 1-7) WFLYDS0013: Started FileSystemDeploymentService for directory /opt/jboss_eap/jboss-eap-7.3/standalone/deployments 2023-03-01 11:25:29,544 INFO [org.wildfly.extension.undertow] (MSC service thread 1-3) WFLYUT0006: Undertow HTTPS listener https listening on [0:0:0:0:0:0:0:0]:8443 2023-03-01 11:25:29,593 INFO [org.jboss.ws.common.management] (MSC service thread 1-5) JBWS022052: Starting JBossWS 5.3.0.Final-redhat-00001 (Apache CXF 3.3.12.redhat-00001) 2023-03-01 11:25:29,674 INFO [org.jboss.as.server] (Controller Boot Thread) WFLYSRV0212: Resuming server 2023-03-01 11:25:29,676 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0025: JBoss EAP 7.3.10.GA (WildFly Core 10.1.25.Final-redhat-00001) started in 2300ms - Started 306 of 560 services (355 services are lazy, passive or on-demand) …&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notes:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt; &lt;p&gt;By default, the collection installs the latest major version of JBoss EAP (7.4.0). As we will discuss upgrading to the next major version in the second part of this article, we have purposely installed the previous major version of the JEE server.&lt;/p&gt; &lt;/li&gt; &lt;li aria-level="1"&gt;If the playbook is run again, no changes are reported, as the collection ensures that the setup of JBoss EAP is idempotent.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Upgrading to the next major version of JBoss EAP&lt;/h2&gt; &lt;p&gt;Minor upgrades contain only fixes. The actual changes are minimal. Features themselves are not modified, and no new ones are added unless a security fix requires it. In short, this means users can perform such updates on their system without fearing side effects or issues for the hosted applications. However, upgrading JBoss EAP to the next major version is a completely different scenario.&lt;/p&gt; &lt;p&gt;Let’s consider the following requirement. The JBoss EAP instances previously installed using Ansible have been targeted to be upgraded to 7.4 with the latest available minor version (7.4.10). Teams behind the hosted apps have tested and confirmed that no code changes are required; however, a plan is already in place if something goes wrong during the upgrade. Any instance having issues during the upgrade needs to downgrade and resume running the previous version.&lt;/p&gt; &lt;p&gt;Let’s build a playbook implementing such a strategy. First, it will stop the currently running instance of JBoss EAP on the target, then install and start the new version, using the same configuration template as the previous server. And, if anything goes wrong during this process, the existing service will be started.&lt;/p&gt; &lt;p&gt;Here is the playbook we will use to perform this migration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: "Update EAP to latest {{ eap_patch_version }}"   hosts: eap_servers   vars: eap_offline_install: True eap_apply_cp: True eap_patch_version: 7.4.9 eap_config_base: 'standalone.xml' eap_instance_name: eap74   collections: - redhat.eap   roles: - eap_systemd   pre_tasks: - name: "Ensure previous version of EAP is not running"    ansible.builtin.service:      name: eap73      state: stopped   tasks: - block:      - name: "Perform EAP {{ eap_patch_version }} installation"        ansible.builtin.include_role:          name: eap_install      - name: "Ensure EAP service is deployed and running."        ansible.builtin.include_role:          name: eap_systemd      - name: "Ensure EAP service is functional"        ansible.builtin.include_role:          name: eap_validation    rescue:      - name: "EAP upgrade failed, fallback to previous version"        ansible.builtin.service:          name: eap73          state: running&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The playbook above implements the migration process we described above. First, it stops the existing JBoss EAP server running on the target. Then, it leverages the &lt;code&gt;redhat.eap&lt;/code&gt; collection again to install the new version and start it as a service. To ensure that this instance is functional, it runs the &lt;code&gt;eap_validation&lt;/code&gt; role, also provided by the collection, that performs some basic sanity checks against the service.&lt;/p&gt; &lt;p&gt;Assuming nothing has failed during those three steps, the new server is running, and the migration has been completed successfully. If anything goes wrong, the new server is shut down, and the previous instance is restarted.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; This playbook is not idempotent and is aimed at being run only for migration purposes. This is both for the sake of simplicity and readability.&lt;/p&gt; &lt;h2&gt;Performing the upgrade and update without downtime&lt;/h2&gt; &lt;p&gt;Most of the time, updating and upgrading JBoss EAP will require restarting the service, which implies some downtime. However, if there is more than one instance of JBoss EAP to update, it is possible to use a powerful Ansible feature to avoid any downtime (assuming there is a smart proxy in front of the JBoss EAP farm, such as &lt;code&gt;httpd&lt;/code&gt; using &lt;code&gt;mod_cluster&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;By default, Ansible will try to connect to any hosts belonging to the &lt;code&gt;eap_servers&lt;/code&gt; group and perform the update and the upgrade in parallel. If everything goes according to plan, there is still a risk of downtime, as the server might be restarting at the same time. If anything goes wrong, most, if not all, will need to roll back to the previous version, leading to even more chances of downtime.&lt;/p&gt; &lt;p&gt;However, this &lt;a href="https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_strategies.html"&gt;execution strategy&lt;/a&gt; can be configured by adding the keyword &lt;code&gt;serial&lt;/code&gt; to the playbook. This will configure how Ansible will operate on the list of servers to connect and execute by batch, only a subset of them at once. With this feature, we can implement the following approach:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Update (or upgrade) on target;&lt;/li&gt; &lt;li aria-level="1"&gt;Update (or upgrade) one-third of the entire farm;&lt;/li&gt; &lt;li aria-level="1"&gt;Update (or upgrade) the rest of the targets.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Such a strategy offers a lot of peace of mind; if the first machine to be targeted fails to update or upgrade, you can stop the process here and investigate what went wrong. The rest of the servers are still running without any risk of downtime. The same applies to the next batch. If something goes wrong with the first third of the servers, then most of the farm is still running the old version, uninterrupted.&lt;/p&gt; &lt;p&gt;To implement this strategy in our upgrade playbook, we only need to add the attribute &lt;code&gt;serial&lt;/code&gt;. We valorize it using a list containing three values, as shown below:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: "Update EAP to latest {{ eap_patch_version }}" hosts: all serial: - 1 - 30% - 70% vars: eap_offline_install: True eap_apply_cp: True eap_patch_version: 7.4.9 eap_instance_name: eap74 collections: - redhat.eap roles: - eap_install - eap_systemd pre_tasks: - name: "Ensure previous version of EAP is not running" ansible.builtin.service: name: eap73 state: stopped tasks: - block: - name: "Perform EAP {{ eap_patch_version }} installation" ansible.builtin.include_role: name: eap_install - name: "Ensure EAP service is deployed and running." ansible.builtin.include_role: name: eap_systemd - name: "Ensure EAP service is functional" ansible.builtin.include_role: name: eap_validation rescue: - name: "EAP upgrade failed, fallback to previous version" ansible.builtin.service: name: eap73 state: running&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the values within the serial property can indeed be a mix of integers (1) and percentages.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;With the heavy lifting of the server installation managed by the &lt;code&gt;redhat.eap&lt;/code&gt; collection and the primitives provided by Ansible, we have implemented a sound strategy to perform updates and upgrades on a potentially very large farm of instances.&lt;/p&gt; &lt;p&gt;The two playbooks we displayed in this article are both short and simple to understand. Their content focuses only on the environment specificity (version executed, execution strategy), and the inner workings of EAP are fully encapsulated inside the collection, which provides peace of mind in what would typically be a complex operation and process.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/04/update-and-upgrade-jboss-eap-ansible" title="Update and upgrade JBoss EAP with Ansible"&gt;Update and upgrade JBoss EAP with Ansible&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Romain Pelisse</dc:creator><dc:date>2023-04-04T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 2.16.6.Final released - Maintenance release</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-16-6-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-16-6-final-released/</id><updated>2023-04-04T00:00:00Z</updated><content type="html">We released Quarkus 2.16.6.Final, the fsixthifth maintenance release of our 2.16 release train. As usual, it contains bugfixes and documentation improvements. It should be a safe upgrade for anyone already using 2.16. If you are not already using 2.16, please refer to our migration guide. Full changelog You can get...</content><dc:creator>Guillaume Smet</dc:creator></entry></feed>
